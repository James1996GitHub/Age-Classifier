{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code Dump.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eX1QRuJqWkRWo82aNugV1xigwkbLT3vI",
      "authorship_tag": "ABX9TyOMzG5FF0ZkeKoy57nxm2ZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/James1996GitHub/Age-Classifier/blob/main/Code_Dump.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0e67eOUwnX3"
      },
      "source": [
        "Mounts the Gdive to the script. This allows us to acccess the files from our Google drive account (import Test/Train data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSSdAMN8qNR-"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k1KNFM0xDkX"
      },
      "source": [
        "A linst of all our imports that we will bw using in this project.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB-OKYGOM8gq"
      },
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import PIL.Image\n",
        "import cv2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NitVID1_OqXa"
      },
      "source": [
        "## Declaring training and testing data as variables for future convienience. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4rgQd1dxM_8",
        "outputId": "9cdf8e67-3779-4c79-8ba5-837ca8a1efa3"
      },
      "source": [
        "TrainDir = '/content/gdrive/MyDrive/train_DETg9GD.zip (Unzipped Files)/Train'\n",
        "TestDir = '/content/gdrive/MyDrive/Actors Age Project /Test/test.zip (Unzipped Files)/Test'\n",
        "NumTrainingData = len(os.listdir(TrainDir))\n",
        "NumTestingData = len(os.listdir(TestDir))\n",
        "print('Total training images:',NumTrainingData) #returns 3 more images than expected.\n",
        "print('Total testing images:',NumTestingData)\n",
        "print('Total images:',NumTestingData + NumTrainingData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training images: 19909\n",
            "Total testing images: 6636\n",
            "Total images: 26545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxfBkYGkhQYk"
      },
      "source": [
        "Building an idea of what the shape of our data pipline will look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "Lg-1lWKQP_75",
        "outputId": "65c3bb47-d5c1-4a2a-f14a-720e952dd493"
      },
      "source": [
        "TrainCSV = '/content/gdrive/MyDrive/train_DETg9GD.zip (Unzipped Files)/train.csv'\n",
        "TestCSV = '/content/gdrive/MyDrive/Actors Age Project /Test/test.zip (Unzipped Files)/test.csv'\n",
        "CSVLables = ['Image File', 'Age Group']\n",
        "TrainDataShape = pd.read_csv(TrainCSV, names=CSVLables, skiprows=[0])\n",
        "TestDataShape = pd.read_csv(TestCSV, names=CSVLables, skiprows=[0])\n",
        "print('Training datashape:',TrainDataShape.shape)\n",
        "print('Testing datashape:',TestDataShape.shape)\n",
        "print('Total rows:',TestDataShape.shape[0]+TrainDataShape.shape[0])\n",
        "TrainDataShape.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training datashape: (19906, 2)\n",
            "Testing datashape: (6636, 2)\n",
            "Total rows: 26542\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image File</th>\n",
              "      <th>Age Group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>377.jpg</td>\n",
              "      <td>MIDDLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17814.jpg</td>\n",
              "      <td>YOUNG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21283.jpg</td>\n",
              "      <td>MIDDLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16496.jpg</td>\n",
              "      <td>YOUNG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4487.jpg</td>\n",
              "      <td>MIDDLE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Image File Age Group\n",
              "0    377.jpg    MIDDLE\n",
              "1  17814.jpg     YOUNG\n",
              "2  21283.jpg    MIDDLE\n",
              "3  16496.jpg     YOUNG\n",
              "4   4487.jpg    MIDDLE"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "U7GADXAnGX1C",
        "outputId": "d514c83d-dc42-44ba-f839-95613e48e9bf"
      },
      "source": [
        "TestDataShape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image File</th>\n",
              "      <th>Age Group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25321.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>989.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19277.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13093.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5367.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6631</th>\n",
              "      <td>1876.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6632</th>\n",
              "      <td>14940.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6633</th>\n",
              "      <td>3638.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6634</th>\n",
              "      <td>376.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6635</th>\n",
              "      <td>9357.jpg</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6636 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image File  Age Group\n",
              "0     25321.jpg        NaN\n",
              "1       989.jpg        NaN\n",
              "2     19277.jpg        NaN\n",
              "3     13093.jpg        NaN\n",
              "4      5367.jpg        NaN\n",
              "...         ...        ...\n",
              "6631   1876.jpg        NaN\n",
              "6632  14940.jpg        NaN\n",
              "6633   3638.jpg        NaN\n",
              "6634    376.jpg        NaN\n",
              "6635   9357.jpg        NaN\n",
              "\n",
              "[6636 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxmoYe4ko_LJ"
      },
      "source": [
        "DATA AUGMENTATION WOULD BE DONE IN THIS STEP IN THIS STAGE using the Keras ImageDataGenerator method/function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDLkJFRRhJHW"
      },
      "source": [
        "Epoch size = Total Train Data /batch size\n",
        "\n",
        "For 19,909 = num training \n",
        "\n",
        "A good batch size = 43\n",
        "\n",
        "epoch = 1607 / 57 = 463\n",
        "\n",
        "For 19906 \n",
        "\n",
        "batch size = 74\n",
        "\n",
        "epoch = 269"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZK5lbYEd8lb"
      },
      "source": [
        "batch_size = 43\n",
        "epochs = 20\n",
        "Image_size = [40,40] # See in build the model section: old value = [23,23]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WltVALQ3VBBn"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XADYBuhgpSHD",
        "outputId": "372d53cf-8136-47df-de10-7a68bab0a84a"
      },
      "source": [
        "train_rescale=ImageDataGenerator(rescale=1./255.,validation_split=0.3)\n",
        "test_rescale=ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "#training generator\n",
        "train_generator=train_rescale.flow_from_dataframe(\n",
        "    dataframe=TrainDataShape,\n",
        "    directory=TrainDir, \n",
        "    x_col=CSVLables[0],\n",
        "    y_col=CSVLables[1],\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    subset = 'training',\n",
        "    class_mode=\"categorical\",\n",
        "    target_size= Image_size)\n",
        "\n",
        "#validation generator\n",
        "validation_generator=train_rescale.flow_from_dataframe(\n",
        "    dataframe=TrainDataShape,\n",
        "    directory=TrainDir,\n",
        "    x_col=CSVLables[0],\n",
        "    y_col=CSVLables[1],\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    subset = 'validation',\n",
        "    class_mode=\"categorical\",\n",
        "    target_size= Image_size)\n",
        "\n",
        "#test generator\n",
        "test_generator=test_rescale.flow_from_dataframe(\n",
        "    dataframe=TestDataShape,\n",
        "    directory=TestDir,\n",
        "    x_col=CSVLables[0],\n",
        "    y_col=None,\n",
        "    batch_size=32,\n",
        "    class_mode=None,\n",
        "    target_size= Image_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13935 validated image filenames belonging to 3 classes.\n",
            "Found 5971 validated image filenames belonging to 3 classes.\n",
            "Found 6636 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV_XSHNJVE1o"
      },
      "source": [
        "Next we are building a Alexnet CNN archetecture.\n",
        "Build the model https://www.tensorflow.org/tutorials/images/classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw2LNVmFVLIJ"
      },
      "source": [
        "num_classes = 3\n",
        "img_height =  40  \n",
        "img_width =  40 \n",
        "\n",
        "model = Sequential([\n",
        "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),  \n",
        "  layers.Conv2D(96,kernel_size=(11,11), padding='same', activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Conv2D(256,kernel_size=(5,5), padding='same', activation='relu'),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(384,kernel_size=(3,3), padding='same', activation='relu'),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Conv2D(384,kernel_size=(3,3), padding='same', activation='relu'),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Conv2D(256,kernel_size=(3,3), padding='same', activation='relu'),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(84, activation='relu'),\n",
        "  layers.Dropout(0.3),\n",
        "  layers.Dense(4096, activation='relu'),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=[tf.keras.metrics.Precision()],\n",
        "    optimizer = 'sgd'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4cpYimYWUDG"
      },
      "source": [
        "Train the model - at the moment this is using the incorrect batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6zc0mrVWW7W",
        "outputId": "f0a9f01a-b488-4c7f-8c8d-21bc6468e1ef"
      },
      "source": [
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "\n",
        "salisburyNet = model.fit_generator(generator=train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=epochs\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "435/435 [==============================] - 4751s 11s/step - loss: 0.8629 - precision: 0.6340 - val_loss: 1.1974 - val_precision: 0.0000e+00\n",
            "Epoch 2/20\n",
            "435/435 [==============================] - 1225s 3s/step - loss: 0.7572 - precision: 0.7001 - val_loss: 0.8544 - val_precision: 0.6258\n",
            "Epoch 3/20\n",
            "435/435 [==============================] - 1219s 3s/step - loss: 0.7025 - precision: 0.7298 - val_loss: 0.7488 - val_precision: 0.6734\n",
            "Epoch 4/20\n",
            "435/435 [==============================] - 1212s 3s/step - loss: 0.6522 - precision: 0.7506 - val_loss: 0.6670 - val_precision: 0.7396\n",
            "Epoch 5/20\n",
            "435/435 [==============================] - 1213s 3s/step - loss: 0.6083 - precision: 0.7675 - val_loss: 0.6969 - val_precision: 0.7174\n",
            "Epoch 6/20\n",
            "435/435 [==============================] - 1215s 3s/step - loss: 0.5630 - precision: 0.7915 - val_loss: 0.6324 - val_precision: 0.7382\n",
            "Epoch 7/20\n",
            "435/435 [==============================] - 1226s 3s/step - loss: 0.5178 - precision: 0.8091 - val_loss: 0.6674 - val_precision: 0.7431\n",
            "Epoch 8/20\n",
            "435/435 [==============================] - 1234s 3s/step - loss: 0.4829 - precision: 0.8175 - val_loss: 0.6060 - val_precision: 0.7658\n",
            "Epoch 9/20\n",
            "435/435 [==============================] - 1222s 3s/step - loss: 0.4337 - precision: 0.8389 - val_loss: 0.6469 - val_precision: 0.7515\n",
            "Epoch 10/20\n",
            "435/435 [==============================] - 1208s 3s/step - loss: 0.4039 - precision: 0.8497 - val_loss: 0.6862 - val_precision: 0.7366\n",
            "Epoch 11/20\n",
            "435/435 [==============================] - 1224s 3s/step - loss: 0.3588 - precision: 0.8673 - val_loss: 0.7054 - val_precision: 0.7376\n",
            "Epoch 12/20\n",
            "435/435 [==============================] - 1219s 3s/step - loss: 0.3157 - precision: 0.8816 - val_loss: 0.6607 - val_precision: 0.7486\n",
            "Epoch 13/20\n",
            "435/435 [==============================] - 1202s 3s/step - loss: 0.2866 - precision: 0.8939 - val_loss: 0.6630 - val_precision: 0.7682\n",
            "Epoch 14/20\n",
            "435/435 [==============================] - 1200s 3s/step - loss: 0.2533 - precision: 0.9072 - val_loss: 0.5788 - val_precision: 0.7853\n",
            "Epoch 15/20\n",
            "435/435 [==============================] - 1230s 3s/step - loss: 0.2310 - precision: 0.9137 - val_loss: 0.5856 - val_precision: 0.7947\n",
            "Epoch 16/20\n",
            "435/435 [==============================] - 1212s 3s/step - loss: 0.2004 - precision: 0.9280 - val_loss: 0.8123 - val_precision: 0.7670\n",
            "Epoch 17/20\n",
            "435/435 [==============================] - 1214s 3s/step - loss: 0.1784 - precision: 0.9353 - val_loss: 0.7504 - val_precision: 0.7541\n",
            "Epoch 18/20\n",
            "435/435 [==============================] - 1238s 3s/step - loss: 0.1633 - precision: 0.9417 - val_loss: 0.7561 - val_precision: 0.7780\n",
            "Epoch 19/20\n",
            "435/435 [==============================] - 1231s 3s/step - loss: 0.1374 - precision: 0.9517 - val_loss: 0.7745 - val_precision: 0.7806\n",
            "Epoch 20/20\n",
            "435/435 [==============================] - 1224s 3s/step - loss: 0.1252 - precision: 0.9564 - val_loss: 0.7096 - val_precision: 0.7942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-exM8vlhFF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81fe79b-735e-45f1-8bf6-78c3246be244"
      },
      "source": [
        "model.save('/content/gdrive/MyDrive/Actors Age Project.h')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Actors Age Project/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpg7GMm4T3KN"
      },
      "source": [
        "salisburyNet = tf.keras.models.load_model('/content/gdrive/MyDrive/Actors Age Project.h')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}